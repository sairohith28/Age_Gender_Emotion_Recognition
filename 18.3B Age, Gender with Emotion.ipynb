{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age, Gender and Emotion Detection\n",
    "\n",
    "### Let's load our classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "import dlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "from contextlib import contextmanager\n",
    "from wide_resnet import WideResNet\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "\n",
    "classifier = load_model('./model/emotion_little_vgg_2.h5')\n",
    "pretrained_model = \"https://github.com/yu4u/age-gender-estimation/releases/download/v0.5/weights.28-3.73.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Age, Gender, Emotion of people in any Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 834ms/step\n",
      "1/1 [==============================] - 1s 664ms/step\n",
      "1/1 [==============================] - 0s 369ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define Image Path Here\n",
    "image_path = \"./images\"\n",
    "\n",
    "modhash = 'fbe63257a054c1c5466cfd7bf14646d6'\n",
    "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "# Define our model parameters\n",
    "depth = 16\n",
    "k = 8\n",
    "weight_file = None\n",
    "margin = 0.4\n",
    "image_dir = None\n",
    "\n",
    "# Get our weight file \n",
    "# if not weight_file:\n",
    "#     weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "#                            file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\n",
    "# load model and weights\n",
    "weight_file=\"weights.28-3.73.hdf5\"\n",
    "img_size = 64\n",
    "model = WideResNet(img_size, depth=depth, k=k)()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "image_names = [f for f in listdir(image_path) if isfile(join(image_path, f))]\n",
    "\n",
    "for image_name in image_names:\n",
    "    frame = cv2.imread(\"./images/\" + image_name)\n",
    "    preprocessed_faces_emo = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
    "            face_gray_emo = img_to_array(face_gray_emo)\n",
    "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
    "            preprocessed_faces_emo.append(face_gray_emo)\n",
    "\n",
    "        # make a prediction for Age and Gender\n",
    "        results = model.predict(np.array(faces))\n",
    "        predicted_genders = results[0]\n",
    "        ages = np.arange(0, 101).reshape(101, 1)\n",
    "        predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "        # make a prediction for Emotion \n",
    "        emo_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
    "            emo_labels.append(emotion_classes[preds.argmax()])\n",
    "        \n",
    "        # draw results\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
    "                                        \"F\" if predicted_genders[i][0] > 0.4 else \"M\", emo_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detector\", frame)\n",
    "    filename = \"output_images/\"+image_name\n",
    "    cv2.imwrite(filename,frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection with webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "# Loop through every frame until we close our webcam\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Show image \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    \n",
    "    # Checks whether q has been hit and stops the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "# Releases the webcam\n",
    "cap.release()\n",
    "# Closes the frame\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 587ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "#import urllib\n",
    "\n",
    "#url='http://192.168.0.100:8080/shot.jpg'\n",
    "\n",
    "# Define Image Path Here\n",
    "image_path = \"./images/\"\n",
    "\n",
    "modhash = 'fbe63257a054c1c5466cfd7bf14646d6'\n",
    "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "# Define our model parameters\n",
    "depth = 16\n",
    "k = 8\n",
    "weight_file = None\n",
    "margin = 0.4\n",
    "image_dir = None\n",
    "\n",
    "# Get our weight file \n",
    "# if not weight_file:\n",
    "#     weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "#                            file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\n",
    "# load model and weights\n",
    "weight_file=\"weights.28-3.73.hdf5\"\n",
    "img_size = 64\n",
    "model = WideResNet(img_size, depth=depth, k=k)()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #imgResp=urllib.request.urlopen(url)\n",
    "    #imgNp=np.array(bytearray(imgResp.read()),dtype=np.uint8)\n",
    "    #img=cv2.imdecode(imgNp,-1)\n",
    "    #frame=img\n",
    "\n",
    "    preprocessed_faces_emo = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
    "            face_gray_emo = img_to_array(face_gray_emo)\n",
    "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
    "            preprocessed_faces_emo.append(face_gray_emo)\n",
    "\n",
    "        # make a prediction for Age and Gender\n",
    "        results = model.predict(np.array(faces))\n",
    "        predicted_genders = results[0]\n",
    "        ages = np.arange(0, 101).reshape(101, 1)\n",
    "        predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "        # make a prediction for Emotion \n",
    "        emo_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
    "            emo_labels.append(emotion_classes[preds.argmax()])\n",
    "        \n",
    "        # draw results\n",
    "        \n",
    "        for i, d in enumerate(detected):\n",
    "            s=predicted_genders[i][0]*100\n",
    "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
    "                                        \"F\" if predicted_genders[i][0] > 0.4 else \"M {:.2f}\".format(s), emo_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detector\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Age, Gender, Emotion of people in any Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 588ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\AGE\\Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System\\18.3B Age, Gender with Emotion.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AGE/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/18.3B%20Age%2C%20Gender%20with%20Emotion.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m input_img \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(frame, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AGE/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/18.3B%20Age%2C%20Gender%20with%20Emotion.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m img_h, img_w, _ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mshape(input_img)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AGE/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/18.3B%20Age%2C%20Gender%20with%20Emotion.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m detected \u001b[39m=\u001b[39m detector(frame, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AGE/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/18.3B%20Age%2C%20Gender%20with%20Emotion.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m faces \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty((\u001b[39mlen\u001b[39m(detected), img_size, img_size, \u001b[39m3\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AGE/Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System/18.3B%20Age%2C%20Gender%20with%20Emotion.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m preprocessed_faces_emo \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "modhash = 'fbe63257a054c1c5466cfd7bf14646d6'\n",
    "emotion_classes = {0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n",
    "\n",
    "def draw_label(image, point, label, font=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "               font_scale=0.8, thickness=1):\n",
    "    size = cv2.getTextSize(label, font, font_scale, thickness)[0]\n",
    "    x, y = point\n",
    "    cv2.rectangle(image, (x, y - size[1]), (x + size[0], y), (255, 0, 0), cv2.FILLED)\n",
    "    cv2.putText(image, label, point, font, font_scale, (255, 255, 255), thickness, lineType=cv2.LINE_AA)\n",
    "    \n",
    "\n",
    "# Define our model parameters\n",
    "depth = 16\n",
    "k = 8\n",
    "weight_file = None\n",
    "margin = 0.4\n",
    "image_dir = None\n",
    "\n",
    "# Get our weight file \n",
    "# if not weight_file:\n",
    "#     weight_file = get_file(\"weights.28-3.73.hdf5\", pretrained_model, cache_subdir=\"pretrained_models\",\n",
    "#                            file_hash=modhash, cache_dir=Path(sys.argv[0]).resolve().parent)\n",
    "weight_file=\"weights.28-3.73.hdf5\"\n",
    "# load model and weights\n",
    "img_size = 64\n",
    "model = WideResNet(img_size, depth=depth, k=k)()\n",
    "model.load_weights(weight_file)\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture('D:\\AGE\\Deep-Surveillance-Monitor-Facial-Emotion-Age-Gender-Recognition-System\\DSC_0029.MOV')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output_video_1.mp4', fourcc, 10.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "\n",
    "    preprocessed_faces_emo = []           \n",
    " \n",
    "    input_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img_h, img_w, _ = np.shape(input_img)\n",
    "    detected = detector(frame, 1)\n",
    "    faces = np.empty((len(detected), img_size, img_size, 3))\n",
    "    \n",
    "    preprocessed_faces_emo = []\n",
    "    if len(detected) > 0:\n",
    "        for i, d in enumerate(detected):\n",
    "            x1, y1, x2, y2, w, h = d.left(), d.top(), d.right() + 1, d.bottom() + 1, d.width(), d.height()\n",
    "            xw1 = max(int(x1 - margin * w), 0)\n",
    "            yw1 = max(int(y1 - margin * h), 0)\n",
    "            xw2 = min(int(x2 + margin * w), img_w - 1)\n",
    "            yw2 = min(int(y2 + margin * h), img_h - 1)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            # cv2.rectangle(img, (xw1, yw1), (xw2, yw2), (255, 0, 0), 2)\n",
    "            faces[i, :, :, :] = cv2.resize(frame[yw1:yw2 + 1, xw1:xw2 + 1, :], (img_size, img_size))\n",
    "            face =  frame[yw1:yw2 + 1, xw1:xw2 + 1, :]\n",
    "            face_gray_emo = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            face_gray_emo = cv2.resize(face_gray_emo, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "            face_gray_emo = face_gray_emo.astype(\"float\") / 255.0\n",
    "            face_gray_emo = img_to_array(face_gray_emo)\n",
    "            face_gray_emo = np.expand_dims(face_gray_emo, axis=0)\n",
    "            preprocessed_faces_emo.append(face_gray_emo)\n",
    "\n",
    "        # make a prediction for Age and Gender\n",
    "        results = model.predict(np.array(faces))\n",
    "        predicted_genders = results[0]\n",
    "        ages = np.arange(0, 101).reshape(101, 1)\n",
    "        predicted_ages = results[1].dot(ages).flatten()\n",
    "\n",
    "        # make a prediction for Emotion \n",
    "        emo_labels = []\n",
    "        for i, d in enumerate(detected):\n",
    "            preds = classifier.predict(preprocessed_faces_emo[i])[0]\n",
    "            emo_labels.append(emotion_classes[preds.argmax()])\n",
    "        \n",
    "        # draw results\n",
    "        for i, d in enumerate(detected):\n",
    "            label = \"{}, {}, {}\".format(int(predicted_ages[i]),\n",
    "                                        \"F\" if predicted_genders[i][0] > 0.4 else \"M\", emo_labels[i])\n",
    "            draw_label(frame, (d.left(), d.top()), label)\n",
    "\n",
    "    out.write(frame)\n",
    "    cv2.imshow(\"Emotion Detector\", frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
